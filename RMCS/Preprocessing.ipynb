{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random"
      ],
      "metadata": {
        "id": "zbu_DY9upRyO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature Selecting\n"
      ],
      "metadata": {
        "id": "mADwXrIqDONC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RB0fQuAzpPag"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/BERT Models/Dataset/reddit.csv')\n",
        "\n",
        "data = list(set(data['sentence']))\n",
        "\n",
        "df = pd.DataFrame()\n",
        "df['sentence'] = data\n",
        "df.to_csv('/content/drive/MyDrive/BERT Models/Dataset/reddit.csv', index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating the dataset for Training"
      ],
      "metadata": {
        "id": "cSYLQH9QDRgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test = random.sample(list(data), k = 10000)\n",
        "\n",
        "print('Finish selecting')\n",
        "print(type(test))\n",
        "print(len(test))\n",
        "\n",
        "def filter_len(dataset):\n",
        "    if len(dataset) >= 15:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "filtering = filter(filter_len, test)\n",
        "# print(f'1: {len(list(filtering))}')\n",
        "newdata = list(filtering)\n",
        "print(type(newdata))\n",
        "print('Finish filtering')\n",
        "print(len(newdata))\n",
        "\n",
        "while(len(newdata) != 10000):\n",
        "    temp = random.choice(list(data))\n",
        "    print(temp)\n",
        "    if temp not in newdata and (len(temp) >= 15):\n",
        "        newdata.append(temp)\n",
        "\n",
        "print('Finish appending')\n",
        "\n",
        "data10k = pd.DataFrame()\n",
        "data10k['sentence'] = newdata\n",
        "data10k.to_csv('/Users/jedai/Desktop/Python/testing/10kdata.csv', index=False)"
      ],
      "metadata": {
        "id": "ivz45OaAp6S0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating the Unbiased Dataset\n"
      ],
      "metadata": {
        "id": "R4pkZOazDTvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "maindata = pd.read_csv('/Users/jedai/Desktop/Python/testing/mergedData.csv')\n",
        "adddata = pd.read_csv('/Users/jedai/Desktop/Python/testing/slang2800.csv')\n",
        "\n",
        "adddata['label'] = 1\n",
        "\n",
        "slang500 = pd.merge(adddata, maindata[maindata['label'] == 1], how = 'outer', on = ['sentence', 'label'])\n",
        "\n",
        "# while len(slang500) != 500:\n",
        "#     random_key = random.choice(slang500.index)\n",
        "#     slang500 = slang500.drop(slang500.index[random_key], axis = 0)\n",
        "\n",
        "cleanSlang = []\n",
        "\n",
        "for i in slang500['sentence']:\n",
        "    if len(i) <= 128:\n",
        "        cleanSlang.append(i)\n",
        "\n",
        "\n",
        "slangtrain400 = []\n",
        "while len(slangtrain400) != 400:\n",
        "    temp = random.choice(cleanSlang)\n",
        "    if temp not in slangtrain400:\n",
        "        slangtrain400.append(temp)\n",
        "\n",
        "print(1)\n",
        "\n",
        "slangtest50 = []\n",
        "while len(slangtest50) != 50:\n",
        "    temp = random.choice(cleanSlang)\n",
        "    # print(temp)\n",
        "    if temp not in slangtrain400 and temp not in slangtest50:\n",
        "        # print(50)\n",
        "        slangtest50.append(temp)\n",
        "\n",
        "normal400 = []\n",
        "test50 = []\n",
        "print(2)\n",
        "\n",
        "maindata = maindata[maindata['label'] == 0]\n",
        "\n",
        "while len(normal400) != 400:\n",
        "    random_key = random.choice(maindata.index)\n",
        "    sentence = maindata.loc[random_key, 'sentence']\n",
        "    if sentence not in normal400 and len(sentence) <= 128:\n",
        "        normal400.append(sentence)\n",
        "\n",
        "print(3)\n",
        "\n",
        "while len(test50) != 50:\n",
        "    random_key = random.choice(maindata.index)\n",
        "    sentence = maindata.loc[random_key, 'sentence']\n",
        "    if sentence not in normal400 and sentence not in test50 and len(sentence) <= 128:\n",
        "        test50.append(sentence)\n",
        "\n",
        "df = pd.DataFrame()\n",
        "df['sentence'] = normal400\n",
        "df['label'] = 0\n",
        "df2 = pd.DataFrame()\n",
        "df2['sentence'] = slangtrain400\n",
        "df2['label'] = 1\n",
        "\n",
        "df3 = pd.DataFrame()\n",
        "df3['sentence'] = slangtest50\n",
        "df3['label'] = 1\n",
        "df4 = pd.DataFrame()\n",
        "df4['sentence'] = test50\n",
        "df4['label'] = 0\n",
        "\n",
        "finalDataTrain = pd.merge(df2, df, how = 'outer', on = ['sentence', 'label'])\n",
        "finalDataTest = pd.merge(df3, df4, how = 'outer', on = ['sentence', 'label'])\n",
        "finalDataTrain.to_csv('/Users/jedai/Desktop/Python/testing/unbiasedDataTrain.csv', index = False)\n",
        "finalDataTest.to_csv('/Users/jedai/Desktop/Python/testing/unbiasedDataTest.csv', index = False)"
      ],
      "metadata": {
        "id": "dKD6y3pgCY_t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}