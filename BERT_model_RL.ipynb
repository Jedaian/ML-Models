{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install plotly\n",
        "!pip install cufflinks"
      ],
      "metadata": {
        "id": "24LrDzawQLy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from gym import spaces\n",
        "from transformers import BertTokenizerFast\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizerFast, DistilBertForSequenceClassification\n",
        "from torch.distributions import Categorical\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ecsBRsRsdgSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbCQLTETOp9V"
      },
      "outputs": [],
      "source": [
        "class LabelingEnv(gym.Env):\n",
        "  def __init__(self, instances, labels):\n",
        "    super(LabelingEnv, self).__init__()\n",
        "    self.instances = instances\n",
        "    self.labels = labels\n",
        "    self.current_instance = 0\n",
        "    self.tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "    encoded = self.tokenizer([self.instances[self.current_instance]], return_tensors='pt', padding='max_length', truncation=True, max_length=128, return_token_type_ids=False)\n",
        "\n",
        "    #define the output of the model\n",
        "    self.action_space = spaces.Discrete(2)\n",
        "    self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(1, 128))\n",
        "\n",
        "  def step(self, action):\n",
        "    reward = 1 if action == self.labels[self.current_instance] else -1\n",
        "    self.current_instance += 1\n",
        "    done = self.current_instance == len(self.instances)\n",
        "    if done:\n",
        "      next_state = None\n",
        "    else:\n",
        "        encoded = self.tokenizer([self.instances[self.current_instance]], return_tensors='pt', padding='max_length', truncation=True, max_length=128, return_token_type_ids=False)\n",
        "        next_state = { 'input_ids': encoded['input_ids'], 'attention_mask': encoded['attention_mask'] }\n",
        "    return next_state, reward, done\n",
        "\n",
        "\n",
        "  def reset(self):\n",
        "    self.current_instance = 0\n",
        "    encoded = self.tokenizer([self.instances[self.current_instance]], return_tensors='pt', padding='max_length', truncation=True, max_length=128, return_token_type_ids=False)\n",
        "    return { 'input_ids': encoded['input_ids'], 'attention_mask': encoded['attention_mask'] }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWgDH7MTX07A",
        "outputId": "aefbe852-d060-44bd-94be-24911e1a480f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pexpect/popen_spawn.py:60: DeprecationWarning: setDaemon() is deprecated, set the daemon attribute instead\n",
            "  self._read_thread.setDaemon(True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading model from BERT\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased') # 2 labels: Slang, No Slang\n",
        "for param in model.base_model.parameters():\n",
        "    param.requires_grad = False\n",
        "tokenizer = BertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "#set up an optimizer\n",
        "optimizer = Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/BERT Models/Dataset/unbiasedDataTrain.csv') #the file directory\n",
        "df.drop_duplicates(subset = ['sentence'], inplace = True)\n",
        "\n",
        "instances = df['sentence'].tolist()\n",
        "labels = df['label'].tolist()\n",
        "\n",
        "#custom envinronment\n",
        "env = LabelingEnv(instances, labels)"
      ],
      "metadata": {
        "id": "EWTZrqdlPC5p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c23742d7-c5b7-487d-fc6a-3a8fef2df069"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
            "The class this function is called from is 'BertTokenizerFast'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "all_rewards = []\n",
        "\n",
        "for fold, (train_index, test_index) in enumerate(kf.split(instances)):\n",
        "    print(f'Starting Fold {fold+1}...')\n",
        "    train_instances = [instances[i] for i in train_index]\n",
        "    train_labels = [labels[i] for i in train_index]\n",
        "    test_instances = [instances[i] for i in test_index]\n",
        "    test_labels = [labels[i] for i in test_index]\n",
        "\n",
        "    # Initialize model and optimizer for each fold\n",
        "    model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased') # 2 labels: Slang, No Slang\n",
        "    for param in model.base_model.parameters():\n",
        "        param.requires_grad = False\n",
        "    model.to(device)\n",
        "    optimizer = Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "    # Create environment with training data\n",
        "    env = LabelingEnv(train_instances, train_labels)\n",
        "\n",
        "    n = 100 #number of epochs\n",
        "    model.train()\n",
        "    fold_rewards = []\n",
        "    for epoch in tqdm(range(n), desc = 'Epochs'):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        epoch_rewards = []\n",
        "        pbar = tqdm(total=len(env.instances), desc=f'Epoch {epoch + 1}', leave=False)\n",
        "        while not done:\n",
        "            if state is not None:\n",
        "                state = {k: v.to(device) for k, v in state.items()}\n",
        "                outputs = model(**state)\n",
        "\n",
        "                #softmax for model output\n",
        "                probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "\n",
        "                #sampling action from the probabilities\n",
        "                dist = Categorical(probs[0])\n",
        "                action = dist.sample()\n",
        "\n",
        "                #train in the environment\n",
        "                new_state, reward, done = env.step(action.item())\n",
        "                epoch_rewards.append(reward)\n",
        "\n",
        "                loss = -dist.log_prob(action) * reward\n",
        "\n",
        "                #backpropagation\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                #updating the state\n",
        "                state = new_state if new_state is not None else None\n",
        "\n",
        "                pbar.update(1)\n",
        "            else:\n",
        "                break\n",
        "        pbar.close()\n",
        "        fold_rewards.append(np.sum(epoch_rewards))\n",
        "        print(f'\\nEpoch {epoch + 1}: Total rewards {np.sum(epoch_rewards)}')\n",
        "\n",
        "    print(f'Validating on Fold {fold+1}...')\n",
        "    env = LabelingEnv(test_instances, test_labels)\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for instance in test_instances:\n",
        "            encoded = tokenizer([instance], return_tensors='pt', padding='max_length', truncation=True, max_length=128, return_token_type_ids=False)\n",
        "            encoded = {k: v.to(device) for k, v in encoded.items()}\n",
        "            outputs = model(**encoded)\n",
        "            _, predicted = torch.max(outputs.logits, dim=1)\n",
        "            preds.append(predicted.item())\n",
        "    all_rewards.append(fold_rewards)\n",
        "    accuracy = accuracy_score(test_labels, preds)\n",
        "    precision = precision_score(test_labels, preds)\n",
        "    recall = recall_score(test_labels, preds)\n",
        "    f1 = f1_score(test_labels, preds)\n",
        "\n",
        "    print(f'Validation results for Fold {fold+1}: Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-score: {f1}\\n')\n"
      ],
      "metadata": {
        "id": "CSa48mXkWZRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "import cufflinks as cf\n",
        "import pandas as pd\n",
        "\n",
        "#assuming all_rewards is a list of lists where each sublist is rewards of one fold\n",
        "all_rewards_df = pd.DataFrame(all_rewards).T #transpose to have each fold as a column\n",
        "\n",
        "moving_avg_rewards = all_rewards_df.rolling(window=10).mean()\n",
        "\n",
        "fig = go.Figure()\n",
        "for fold in range(5):\n",
        "    fig.add_trace(go.Scatter(x=list(range(len(moving_avg_rewards))),\n",
        "                             y=moving_avg_rewards[fold],\n",
        "                             mode='lines',\n",
        "                             name=f'Fold {fold+1}'))\n",
        "\n",
        "fig.update_layout(title='Moving Average Rewards per Epoch for each fold',\n",
        "                   xaxis_title='Epoch',\n",
        "                   yaxis_title='Moving Average Rewards')\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "B3bhXdG9YVfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('/content/drive/MyDrive/BERT Models/BERT RL/model')\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/BERT Models/BERT RL/tokenizer')\n",
        "import pickle\n",
        "\n",
        "with open(\"/content/drive/MyDrive/BERT Models/BERT RL/instances\", \"wb\") as f:\n",
        "    pickle.dump(instances, f)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/BERT Models/BERT RL/labels\", \"wb\") as f:\n",
        "    pickle.dump(labels, f)\n"
      ],
      "metadata": {
        "id": "wVtF1gnfxTMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast, DistilBertForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "model_dir = '/content/drive/MyDrive/BERT Models/BERT RL/model'\n",
        "tokenizer_dir = '/content/drive/MyDrive/BERT Models/BERT RL/tokenizer'\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_dir)\n",
        "tokenizer = BertTokenizerFast.from_pretrained(tokenizer_dir)\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/BERT Models/Dataset/unbiasedDataTest.csv')\n",
        "test_instances = test_df['sentence'].tolist()\n",
        "test_labels = test_df['label'].tolist()\n",
        "\n",
        "env = LabelingEnv(test_instances, test_labels)\n",
        "model.eval()\n",
        "preds = []\n",
        "with torch.no_grad():\n",
        "    for instance in test_instances:\n",
        "        encoded = tokenizer([instance], return_tensors='pt', padding='max_length', truncation=True, max_length=128)\n",
        "        encoded = {k: v.to(device) for k, v in encoded.items()}\n",
        "        outputs = model(**encoded)\n",
        "        _, predicted = torch.max(outputs.logits, dim=1)\n",
        "        preds.append(predicted.item())\n",
        "\n",
        "accuracy = accuracy_score(test_labels, preds)\n",
        "precision = precision_score(test_labels, preds)\n",
        "recall = recall_score(test_labels, preds)\n",
        "f1 = f1_score(test_labels, preds)\n",
        "\n",
        "print(f'Test results: Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-score: {f1}\\n')"
      ],
      "metadata": {
        "id": "E1ruvgPMyH0h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}