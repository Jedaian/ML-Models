{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install plotly\n",
        "!pip install cufflinks"
      ],
      "metadata": {
        "id": "24LrDzawQLy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from gym import spaces\n",
        "from transformers import BertTokenizerFast\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizerFast, DistilBertForSequenceClassification\n",
        "from torch.distributions import Categorical\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ecsBRsRsdgSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbCQLTETOp9V"
      },
      "outputs": [],
      "source": [
        "class LabelingEnv(gym.Env):\n",
        "  def __init__(self, instances, labels):\n",
        "    super(LabelingEnv, self).__init__()\n",
        "    self.instances = instances\n",
        "    self.labels = labels\n",
        "    self.current_instance = 0\n",
        "    self.tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "    encoded = self.tokenizer([self.instances[self.current_instance]], return_tensors='pt', padding='max_length', truncation=True, max_length=128, return_token_type_ids=False)\n",
        "\n",
        "    #define the output of the model\n",
        "    self.action_space = spaces.Discrete(2)\n",
        "    self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(1, 128))\n",
        "\n",
        "  def step(self, action):\n",
        "    reward = 1 if action == self.labels[self.current_instance] else -1\n",
        "    self.current_instance += 1\n",
        "    done = self.current_instance == len(self.instances)\n",
        "    if done:\n",
        "      next_state = None\n",
        "    else:\n",
        "        encoded = self.tokenizer([self.instances[self.current_instance]], return_tensors='pt', padding='max_length', truncation=True, max_length=128, return_token_type_ids=False)\n",
        "        next_state = { 'input_ids': encoded['input_ids'], 'attention_mask': encoded['attention_mask'] }\n",
        "    return next_state, reward, done\n",
        "\n",
        "\n",
        "  def reset(self):\n",
        "    self.current_instance = 0\n",
        "    encoded = self.tokenizer([self.instances[self.current_instance]], return_tensors='pt', padding='max_length', truncation=True, max_length=128, return_token_type_ids=False)\n",
        "    return { 'input_ids': encoded['input_ids'], 'attention_mask': encoded['attention_mask'] }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWgDH7MTX07A",
        "outputId": "aefbe852-d060-44bd-94be-24911e1a480f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pexpect/popen_spawn.py:60: DeprecationWarning: setDaemon() is deprecated, set the daemon attribute instead\n",
            "  self._read_thread.setDaemon(True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading model from BERT\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased') # 2 labels: Slang, No Slang\n",
        "for param in model.base_model.parameters():\n",
        "    param.requires_grad = False\n",
        "tokenizer = BertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "#set up an optimizer\n",
        "optimizer = Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/BERT Models/Dataset/unbiasedDataTrain.csv') #the file directory\n",
        "df.drop_duplicates(subset = ['sentence'], inplace = True)\n",
        "\n",
        "instances = df['sentence'].tolist()\n",
        "labels = df['label'].tolist()\n",
        "\n",
        "#custom envinronment\n",
        "env = LabelingEnv(instances, labels)"
      ],
      "metadata": {
        "id": "EWTZrqdlPC5p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c23742d7-c5b7-487d-fc6a-3a8fef2df069"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
            "The class this function is called from is 'BertTokenizerFast'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "all_rewards = []\n",
        "\n",
        "for fold, (train_index, test_index) in enumerate(kf.split(instances)):\n",
        "    print(f'Starting Fold {fold+1}...')\n",
        "    train_instances = [instances[i] for i in train_index]\n",
        "    train_labels = [labels[i] for i in train_index]\n",
        "    test_instances = [instances[i] for i in test_index]\n",
        "    test_labels = [labels[i] for i in test_index]\n",
        "\n",
        "    # Initialize model and optimizer for each fold\n",
        "    model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased') # 2 labels: Slang, No Slang\n",
        "    for param in model.base_model.parameters():\n",
        "        param.requires_grad = False\n",
        "    model.to(device)\n",
        "    optimizer = Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "    # Create environment with training data\n",
        "    env = LabelingEnv(train_instances, train_labels)\n",
        "\n",
        "    n = 100 #number of epochs\n",
        "    model.train()\n",
        "    fold_rewards = []\n",
        "    for epoch in tqdm(range(n), desc = 'Epochs'):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        epoch_rewards = []\n",
        "        pbar = tqdm(total=len(env.instances), desc=f'Epoch {epoch + 1}', leave=False)\n",
        "        while not done:\n",
        "            if state is not None:\n",
        "                state = {k: v.to(device) for k, v in state.items()}\n",
        "                outputs = model(**state)\n",
        "\n",
        "                #softmax for model output\n",
        "                probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "\n",
        "                #sampling action from the probabilities\n",
        "                dist = Categorical(probs[0])\n",
        "                action = dist.sample()\n",
        "\n",
        "                #train in the environment\n",
        "                new_state, reward, done = env.step(action.item())\n",
        "                epoch_rewards.append(reward)\n",
        "\n",
        "                loss = -dist.log_prob(action) * reward\n",
        "\n",
        "                #backpropagation\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                #updating the state\n",
        "                state = new_state if new_state is not None else None\n",
        "\n",
        "                pbar.update(1)\n",
        "            else:\n",
        "                break\n",
        "        pbar.close()\n",
        "        fold_rewards.append(np.sum(epoch_rewards))\n",
        "        print(f'\\nEpoch {epoch + 1}: Total rewards {np.sum(epoch_rewards)}')\n",
        "\n",
        "    print(f'Validating on Fold {fold+1}...')\n",
        "    env = LabelingEnv(test_instances, test_labels)\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for instance in test_instances:\n",
        "            encoded = tokenizer([instance], return_tensors='pt', padding='max_length', truncation=True, max_length=128, return_token_type_ids=False)\n",
        "            encoded = {k: v.to(device) for k, v in encoded.items()}\n",
        "            outputs = model(**encoded)\n",
        "            _, predicted = torch.max(outputs.logits, dim=1)\n",
        "            preds.append(predicted.item())\n",
        "    all_rewards.append(fold_rewards)\n",
        "    accuracy = accuracy_score(test_labels, preds)\n",
        "    precision = precision_score(test_labels, preds)\n",
        "    recall = recall_score(test_labels, preds)\n",
        "    f1 = f1_score(test_labels, preds)\n",
        "\n",
        "    print(f'Validation results for Fold {fold+1}: Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-score: {f1}\\n')\n"
      ],
      "metadata": {
        "id": "CSa48mXkWZRF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6d421b50-8460-41f6-d23b-3f5741ba902b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Fold 1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "\n",
            "Epoch 1:   0%|          | 0/640 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "                                                            \u001b[A\n",
            "\n",
            "Epoch 1:   1%|▏         | 9/640 [00:00<00:07, 83.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:   3%|▎         | 19/640 [00:00<00:06, 91.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:   5%|▍         | 31/640 [00:00<00:06, 101.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:   7%|▋         | 42/640 [00:00<00:05, 102.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:   8%|▊         | 53/640 [00:00<00:05, 104.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  10%|█         | 64/640 [00:00<00:05, 105.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  12%|█▏        | 75/640 [00:00<00:05, 103.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  14%|█▎        | 87/640 [00:00<00:05, 106.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  15%|█▌        | 99/640 [00:00<00:04, 108.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  17%|█▋        | 111/640 [00:01<00:04, 110.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  19%|█▉        | 124/640 [00:01<00:04, 114.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  21%|██▏       | 136/640 [00:01<00:04, 115.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  23%|██▎       | 148/640 [00:01<00:04, 115.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  25%|██▌       | 160/640 [00:01<00:04, 114.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  27%|██▋       | 172/640 [00:01<00:04, 115.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  29%|██▉       | 184/640 [00:01<00:03, 115.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  31%|███       | 196/640 [00:01<00:03, 112.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  32%|███▎      | 208/640 [00:01<00:03, 114.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  34%|███▍      | 220/640 [00:01<00:03, 113.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  36%|███▋      | 232/640 [00:02<00:03, 113.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  38%|███▊      | 244/640 [00:02<00:03, 113.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  40%|████      | 256/640 [00:02<00:03, 112.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  42%|████▏     | 268/640 [00:02<00:03, 112.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  44%|████▍     | 280/640 [00:02<00:03, 113.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  46%|████▌     | 292/640 [00:02<00:03, 112.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  48%|████▊     | 304/640 [00:02<00:02, 113.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  49%|████▉     | 316/640 [00:02<00:02, 111.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  51%|█████▏    | 328/640 [00:02<00:02, 112.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  53%|█████▎    | 340/640 [00:03<00:02, 113.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  55%|█████▌    | 353/640 [00:03<00:02, 115.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  57%|█████▋    | 365/640 [00:03<00:02, 116.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  59%|█████▉    | 377/640 [00:03<00:02, 115.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  61%|██████    | 389/640 [00:03<00:02, 114.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  63%|██████▎   | 401/640 [00:03<00:02, 115.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  65%|██████▍   | 413/640 [00:03<00:02, 110.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  67%|██████▋   | 426/640 [00:03<00:01, 113.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  68%|██████▊   | 438/640 [00:03<00:01, 108.98it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  70%|███████   | 449/640 [00:04<00:01, 106.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  72%|███████▏  | 460/640 [00:04<00:01, 101.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  74%|███████▎  | 471/640 [00:04<00:01, 101.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  75%|███████▌  | 482/640 [00:04<00:01, 101.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  77%|███████▋  | 493/640 [00:04<00:01, 100.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  79%|███████▉  | 504/640 [00:04<00:01, 100.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  80%|████████  | 515/640 [00:04<00:01, 100.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  82%|████████▏ | 526/640 [00:04<00:01, 103.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  84%|████████▍ | 537/640 [00:04<00:01, 102.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  86%|████████▌ | 549/640 [00:05<00:00, 106.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  88%|████████▊ | 561/640 [00:05<00:00, 107.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  90%|████████▉ | 573/640 [00:05<00:00, 109.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  91%|█████████▏| 585/640 [00:05<00:00, 110.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  93%|█████████▎| 597/640 [00:05<00:00, 111.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  95%|█████████▌| 609/640 [00:05<00:00, 111.98it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  97%|█████████▋| 621/640 [00:05<00:00, 112.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epoch 1:  99%|█████████▉| 633/640 [00:05<00:00, 110.94it/s]\u001b[A\u001b[A\n",
            "\n",
            "Epochs:   1%|          | 1/100 [00:05<09:39,  5.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: Total rewards 34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 2:   0%|          | 0/640 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:   2%|▏         | 12/640 [00:00<00:05, 112.13it/s]\u001b[A\n",
            "Epoch 2:   4%|▍         | 24/640 [00:00<00:05, 109.13it/s]\u001b[A\n",
            "Epoch 2:   6%|▌         | 36/640 [00:00<00:05, 110.35it/s]\u001b[A\n",
            "Epoch 2:   8%|▊         | 48/640 [00:00<00:05, 111.70it/s]\u001b[A\n",
            "Epoch 2:   9%|▉         | 60/640 [00:00<00:05, 110.99it/s]\u001b[A\n",
            "Epoch 2:  11%|█▏        | 72/640 [00:00<00:05, 113.01it/s]\u001b[A\n",
            "Epoch 2:  13%|█▎        | 84/640 [00:00<00:04, 113.70it/s]\u001b[A\n",
            "Epoch 2:  15%|█▌        | 96/640 [00:00<00:04, 113.91it/s]\u001b[A\n",
            "Epoch 2:  17%|█▋        | 108/640 [00:00<00:04, 115.56it/s]\u001b[A\n",
            "Epoch 2:  19%|█▉        | 120/640 [00:01<00:04, 112.67it/s]\u001b[A\n",
            "Epoch 2:  21%|██        | 132/640 [00:01<00:04, 113.99it/s]\u001b[A\n",
            "Epoch 2:  22%|██▎       | 144/640 [00:01<00:04, 113.72it/s]\u001b[A\n",
            "Epoch 2:  24%|██▍       | 156/640 [00:01<00:04, 113.89it/s]\u001b[A\n",
            "Epoch 2:  26%|██▋       | 168/640 [00:01<00:04, 114.05it/s]\u001b[A\n",
            "Epoch 2:  28%|██▊       | 180/640 [00:01<00:04, 114.23it/s]\u001b[A\n",
            "Epoch 2:  30%|███       | 192/640 [00:01<00:03, 115.72it/s]\u001b[A\n",
            "Epoch 2:  32%|███▏      | 204/640 [00:01<00:03, 110.87it/s]\u001b[A\n",
            "Epoch 2:  34%|███▍      | 216/640 [00:01<00:04, 101.90it/s]\u001b[A\n",
            "Epoch 2:  35%|███▌      | 227/640 [00:02<00:04, 95.67it/s] \u001b[A\n",
            "Epoch 2:  37%|███▋      | 237/640 [00:02<00:04, 91.74it/s]\u001b[A\n",
            "Epoch 2:  39%|███▊      | 247/640 [00:02<00:04, 91.78it/s]\u001b[A\n",
            "Epoch 2:  40%|████      | 257/640 [00:02<00:04, 90.95it/s]\u001b[A\n",
            "Epoch 2:  42%|████▏     | 267/640 [00:02<00:04, 91.34it/s]\u001b[A\n",
            "Epoch 2:  43%|████▎     | 277/640 [00:02<00:03, 91.33it/s]\u001b[A\n",
            "Epoch 2:  45%|████▍     | 287/640 [00:02<00:03, 90.87it/s]\u001b[A\n",
            "Epoch 2:  46%|████▋     | 297/640 [00:02<00:03, 91.07it/s]\u001b[A\n",
            "Epoch 2:  48%|████▊     | 307/640 [00:02<00:03, 89.92it/s]\u001b[A\n",
            "Epoch 2:  50%|████▉     | 317/640 [00:03<00:03, 88.92it/s]\u001b[A\n",
            "Epoch 2:  51%|█████     | 326/640 [00:03<00:03, 83.99it/s]\u001b[A\n",
            "Epoch 2:  52%|█████▏    | 335/640 [00:03<00:03, 82.79it/s]\u001b[A\n",
            "Epoch 2:  54%|█████▍    | 344/640 [00:03<00:03, 82.81it/s]\u001b[A\n",
            "Epoch 2:  55%|█████▌    | 353/640 [00:03<00:03, 82.89it/s]\u001b[A\n",
            "Epoch 2:  57%|█████▋    | 362/640 [00:03<00:03, 83.79it/s]\u001b[A\n",
            "Epoch 2:  58%|█████▊    | 372/640 [00:03<00:03, 86.20it/s]\u001b[A\n",
            "Epoch 2:  60%|█████▉    | 381/640 [00:03<00:02, 86.97it/s]\u001b[A\n",
            "Epoch 2:  61%|██████    | 391/640 [00:03<00:02, 88.45it/s]\u001b[A\n",
            "Epoch 2:  62%|██████▎   | 400/640 [00:04<00:02, 87.31it/s]\u001b[A\n",
            "Epoch 2:  64%|██████▍   | 409/640 [00:04<00:02, 84.38it/s]\u001b[A\n",
            "Epoch 2:  65%|██████▌   | 418/640 [00:04<00:02, 84.65it/s]\u001b[A\n",
            "Epoch 2:  67%|██████▋   | 428/640 [00:04<00:02, 86.82it/s]\u001b[A\n",
            "Epoch 2:  68%|██████▊   | 437/640 [00:04<00:02, 85.86it/s]\u001b[A\n",
            "Epoch 2:  70%|██████▉   | 447/640 [00:04<00:02, 87.73it/s]\u001b[A\n",
            "Epoch 2:  71%|███████▏  | 457/640 [00:04<00:02, 90.46it/s]\u001b[A\n",
            "Epoch 2:  73%|███████▎  | 467/640 [00:04<00:01, 90.34it/s]\u001b[A\n",
            "Epoch 2:  75%|███████▍  | 477/640 [00:04<00:01, 88.63it/s]\u001b[A\n",
            "Epoch 2:  76%|███████▌  | 486/640 [00:05<00:01, 87.52it/s]\u001b[A\n",
            "Epoch 2:  77%|███████▋  | 495/640 [00:05<00:01, 86.97it/s]\u001b[A\n",
            "Epoch 2:  79%|███████▉  | 504/640 [00:05<00:01, 84.00it/s]\u001b[A\n",
            "Epoch 2:  80%|████████  | 513/640 [00:05<00:01, 81.18it/s]\u001b[A\n",
            "Epoch 2:  82%|████████▏ | 522/640 [00:05<00:01, 79.54it/s]\u001b[A\n",
            "Epoch 2:  83%|████████▎ | 530/640 [00:05<00:01, 76.53it/s]\u001b[A\n",
            "Epoch 2:  84%|████████▍ | 538/640 [00:05<00:01, 75.00it/s]\u001b[A\n",
            "Epoch 2:  85%|████████▌ | 546/640 [00:05<00:01, 75.76it/s]\u001b[A\n",
            "Epoch 2:  87%|████████▋ | 554/640 [00:05<00:01, 76.34it/s]\u001b[A\n",
            "Epoch 2:  88%|████████▊ | 562/640 [00:06<00:01, 75.59it/s]\u001b[A\n",
            "Epoch 2:  89%|████████▉ | 570/640 [00:06<00:00, 74.22it/s]\u001b[A\n",
            "Epoch 2:  90%|█████████ | 578/640 [00:06<00:00, 73.20it/s]\u001b[A\n",
            "Epoch 2:  92%|█████████▏| 586/640 [00:06<00:00, 74.05it/s]\u001b[A\n",
            "Epoch 2:  93%|█████████▎| 594/640 [00:06<00:00, 75.07it/s]\u001b[A\n",
            "Epoch 2:  94%|█████████▍| 602/640 [00:06<00:00, 75.19it/s]\u001b[A\n",
            "Epoch 2:  95%|█████████▌| 610/640 [00:06<00:00, 75.23it/s]\u001b[A\n",
            "Epoch 2:  97%|█████████▋| 619/640 [00:06<00:00, 78.35it/s]\u001b[A\n",
            "Epoch 2:  99%|█████████▉| 632/640 [00:06<00:00, 91.85it/s]\u001b[A\n",
            "Epochs:   2%|▏         | 2/100 [00:12<10:39,  6.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2: Total rewards -12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 3:   0%|          | 0/640 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3:   2%|▏         | 13/640 [00:00<00:04, 127.39it/s]\u001b[A\n",
            "Epoch 3:   4%|▍         | 26/640 [00:00<00:05, 120.14it/s]\u001b[A\n",
            "Epoch 3:   6%|▌         | 39/640 [00:00<00:05, 114.58it/s]\u001b[A\n",
            "Epoch 3:   8%|▊         | 51/640 [00:00<00:05, 114.27it/s]\u001b[A\n",
            "Epoch 3:  10%|▉         | 63/640 [00:00<00:05, 114.52it/s]\u001b[A\n",
            "Epoch 3:  12%|█▏        | 75/640 [00:00<00:04, 113.45it/s]\u001b[A\n",
            "Epoch 3:  14%|█▎        | 87/640 [00:00<00:04, 111.01it/s]\u001b[A\n",
            "Epoch 3:  15%|█▌        | 99/640 [00:00<00:04, 108.60it/s]\u001b[A\n",
            "Epoch 3:  17%|█▋        | 111/640 [00:00<00:04, 111.84it/s]\u001b[A\n",
            "Epoch 3:  19%|█▉        | 123/640 [00:01<00:04, 113.37it/s]\u001b[A\n",
            "Epoch 3:  21%|██        | 135/640 [00:01<00:04, 112.88it/s]\u001b[A\n",
            "Epoch 3:  23%|██▎       | 147/640 [00:01<00:04, 110.01it/s]\u001b[A\n",
            "Epoch 3:  25%|██▍       | 159/640 [00:01<00:04, 109.07it/s]\u001b[A\n",
            "Epoch 3:  27%|██▋       | 171/640 [00:01<00:04, 111.14it/s]\u001b[A\n",
            "Epoch 3:  29%|██▊       | 183/640 [00:01<00:04, 112.16it/s]\u001b[A\n",
            "Epoch 3:  30%|███       | 195/640 [00:01<00:03, 113.71it/s]\u001b[A\n",
            "Epoch 3:  32%|███▏      | 207/640 [00:01<00:03, 111.63it/s]\u001b[A\n",
            "Epoch 3:  34%|███▍      | 219/640 [00:01<00:03, 110.05it/s]\u001b[A\n",
            "Epoch 3:  36%|███▌      | 231/640 [00:02<00:03, 111.82it/s]\u001b[A\n",
            "Epoch 3:  38%|███▊      | 243/640 [00:02<00:03, 113.56it/s]\u001b[A\n",
            "Epoch 3:  40%|███▉      | 255/640 [00:02<00:03, 115.15it/s]\u001b[A\n",
            "Epoch 3:  42%|████▏     | 267/640 [00:02<00:03, 115.17it/s]\u001b[A\n",
            "Epoch 3:  44%|████▎     | 279/640 [00:02<00:03, 105.67it/s]\u001b[A\n",
            "Epoch 3:  45%|████▌     | 291/640 [00:02<00:03, 108.15it/s]\u001b[A\n",
            "Epoch 3:  47%|████▋     | 303/640 [00:02<00:03, 110.23it/s]\u001b[A\n",
            "Epoch 3:  49%|████▉     | 315/640 [00:02<00:02, 111.50it/s]\u001b[A\n",
            "Epoch 3:  51%|█████     | 327/640 [00:02<00:02, 113.13it/s]\u001b[A\n",
            "Epoch 3:  53%|█████▎    | 339/640 [00:03<00:02, 111.25it/s]\u001b[A\n",
            "Epoch 3:  55%|█████▍    | 351/640 [00:03<00:02, 105.27it/s]\u001b[A\n",
            "Epoch 3:  57%|█████▋    | 363/640 [00:03<00:02, 107.75it/s]\u001b[A\n",
            "Epoch 3:  58%|█████▊    | 374/640 [00:03<00:04, 62.78it/s] \u001b[A\n",
            "Epoch 3:  60%|█████▉    | 383/640 [00:03<00:04, 52.85it/s]\u001b[A\n",
            "Epoch 3:  61%|██████    | 391/640 [00:04<00:04, 51.54it/s]\u001b[A\n",
            "Epoch 3:  62%|██████▏   | 398/640 [00:04<00:04, 51.82it/s]\u001b[A\n",
            "Epoch 3:  64%|██████▍   | 410/640 [00:04<00:03, 64.44it/s]\u001b[A\n",
            "Epoch 3:  66%|██████▌   | 422/640 [00:04<00:02, 75.29it/s]\u001b[A\n",
            "Epoch 3:  68%|██████▊   | 434/640 [00:04<00:02, 84.92it/s]\u001b[A\n",
            "Epoch 3:  69%|██████▉   | 444/640 [00:04<00:02, 76.61it/s]\u001b[A\n",
            "Epoch 3:  71%|███████   | 453/640 [00:05<00:03, 50.70it/s]\u001b[A\n",
            "Epoch 3:  72%|███████▏  | 460/640 [00:05<00:04, 44.75it/s]\u001b[A\n",
            "Epoch 3:  73%|███████▎  | 466/640 [00:05<00:03, 44.85it/s]\u001b[A\n",
            "Epoch 3:  74%|███████▍  | 472/640 [00:05<00:03, 44.73it/s]\u001b[A\n",
            "Epoch 3:  76%|███████▌  | 484/640 [00:05<00:02, 59.39it/s]\u001b[A\n",
            "Epoch 3:  78%|███████▊  | 497/640 [00:05<00:01, 73.77it/s]\u001b[A\n",
            "Epoch 3:  80%|███████▉  | 509/640 [00:05<00:01, 84.10it/s]\u001b[A\n",
            "Epoch 3:  81%|████████▏ | 520/640 [00:05<00:01, 88.25it/s]\u001b[A\n",
            "Epoch 3:  83%|████████▎ | 530/640 [00:06<00:01, 59.40it/s]\u001b[A\n",
            "Epoch 3:  84%|████████▍ | 538/640 [00:06<00:01, 51.97it/s]\u001b[A\n",
            "Epoch 3:  85%|████████▌ | 545/640 [00:06<00:01, 48.24it/s]\u001b[A\n",
            "Epoch 3:  86%|████████▌ | 551/640 [00:06<00:01, 47.22it/s]\u001b[A\n",
            "Epoch 3:  88%|████████▊ | 560/640 [00:06<00:01, 55.48it/s]\u001b[A\n",
            "Epoch 3:  89%|████████▉ | 572/640 [00:06<00:00, 69.40it/s]\u001b[A\n",
            "Epoch 3:  91%|█████████▏| 584/640 [00:07<00:00, 80.89it/s]\u001b[A\n",
            "Epoch 3:  93%|█████████▎| 597/640 [00:07<00:00, 92.12it/s]\u001b[A\n",
            "Epoch 3:  95%|█████████▌| 610/640 [00:07<00:00, 100.17it/s]\u001b[A\n",
            "Epoch 3:  97%|█████████▋| 622/640 [00:07<00:00, 105.30it/s]\u001b[A\n",
            "Epoch 3:  99%|█████████▉| 634/640 [00:07<00:00, 106.09it/s]\u001b[A\n",
            "Epochs:   3%|▎         | 3/100 [00:20<11:18,  7.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3: Total rewards -28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 4:   0%|          | 0/640 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:   2%|▏         | 13/640 [00:00<00:05, 124.63it/s]\u001b[A\n",
            "Epoch 4:   4%|▍         | 26/640 [00:00<00:05, 120.12it/s]\u001b[A\n",
            "Epoch 4:   6%|▌         | 39/640 [00:00<00:04, 120.52it/s]\u001b[A\n",
            "Epoch 4:   8%|▊         | 52/640 [00:00<00:04, 117.66it/s]\u001b[A\n",
            "Epoch 4:  10%|█         | 64/640 [00:00<00:04, 117.80it/s]\u001b[A\n",
            "Epoch 4:  12%|█▏        | 76/640 [00:00<00:04, 117.09it/s]\u001b[A\n",
            "Epoch 4:  14%|█▍        | 88/640 [00:00<00:04, 117.20it/s]\u001b[A\n",
            "Epoch 4:  16%|█▌        | 101/640 [00:00<00:04, 118.99it/s]\u001b[A\n",
            "Epoch 4:  18%|█▊        | 113/640 [00:00<00:04, 114.76it/s]\u001b[A\n",
            "Epoch 4:  20%|█▉        | 125/640 [00:01<00:04, 115.06it/s]\u001b[A\n",
            "Epoch 4:  21%|██▏       | 137/640 [00:01<00:04, 115.96it/s]\u001b[A\n",
            "Epoch 4:  23%|██▎       | 149/640 [00:01<00:04, 116.17it/s]\u001b[A\n",
            "Epoch 4:  25%|██▌       | 161/640 [00:01<00:04, 116.57it/s]\u001b[A\n",
            "Epoch 4:  27%|██▋       | 173/640 [00:01<00:04, 115.09it/s]\u001b[A\n",
            "Epoch 4:  29%|██▉       | 185/640 [00:01<00:03, 116.07it/s]\u001b[A\n",
            "Epoch 4:  31%|███       | 197/640 [00:01<00:03, 114.15it/s]\u001b[A\n",
            "Epoch 4:  33%|███▎      | 209/640 [00:01<00:03, 112.35it/s]\u001b[A\n",
            "Epoch 4:  35%|███▍      | 221/640 [00:01<00:03, 110.12it/s]\u001b[A\n",
            "Epoch 4:  36%|███▋      | 233/640 [00:02<00:03, 111.38it/s]\u001b[A\n",
            "Epoch 4:  38%|███▊      | 245/640 [00:02<00:03, 113.70it/s]\u001b[A\n",
            "Epoch 4:  40%|████      | 257/640 [00:02<00:03, 114.57it/s]\u001b[A\n",
            "Epoch 4:  42%|████▏     | 269/640 [00:02<00:03, 102.34it/s]\u001b[A\n",
            "Epoch 4:  44%|████▍     | 280/640 [00:02<00:03, 97.07it/s] \u001b[A\n",
            "Epoch 4:  45%|████▌     | 290/640 [00:02<00:03, 94.16it/s]\u001b[A\n",
            "Epoch 4:  47%|████▋     | 300/640 [00:02<00:03, 92.36it/s]\u001b[A\n",
            "Epoch 4:  48%|████▊     | 310/640 [00:02<00:03, 91.63it/s]\u001b[A\n",
            "Epoch 4:  50%|█████     | 320/640 [00:02<00:03, 88.67it/s]\u001b[A\n",
            "Epoch 4:  51%|█████▏    | 329/640 [00:03<00:03, 87.32it/s]\u001b[A\n",
            "Epoch 4:  53%|█████▎    | 338/640 [00:03<00:03, 85.26it/s]\u001b[A\n",
            "Epoch 4:  54%|█████▍    | 347/640 [00:03<00:03, 86.27it/s]\u001b[A\n",
            "Epoch 4:  56%|█████▌    | 356/640 [00:03<00:03, 86.78it/s]\u001b[A\n",
            "Epoch 4:  57%|█████▋    | 365/640 [00:03<00:03, 87.58it/s]\u001b[A\n",
            "Epoch 4:  58%|█████▊    | 374/640 [00:03<00:03, 85.81it/s]\u001b[A\n",
            "Epoch 4:  60%|█████▉    | 383/640 [00:03<00:03, 84.25it/s]\u001b[A\n",
            "Epoch 4:  61%|██████▏   | 392/640 [00:03<00:03, 81.58it/s]\u001b[A\n",
            "Epoch 4:  63%|██████▎   | 401/640 [00:03<00:02, 83.41it/s]\u001b[A\n",
            "Epoch 4:  64%|██████▍   | 410/640 [00:04<00:02, 81.74it/s]\u001b[A\n",
            "Epoch 4:  65%|██████▌   | 419/640 [00:04<00:02, 83.18it/s]\u001b[A\n",
            "Epoch 4:  67%|██████▋   | 428/640 [00:04<00:02, 80.96it/s]\u001b[A\n",
            "Epoch 4:  68%|██████▊   | 437/640 [00:04<00:02, 82.26it/s]\u001b[A\n",
            "Epoch 4:  70%|██████▉   | 446/640 [00:04<00:02, 83.71it/s]\u001b[A\n",
            "Epoch 4:  71%|███████   | 455/640 [00:04<00:02, 83.41it/s]\u001b[A\n",
            "Epoch 4:  73%|███████▎  | 465/640 [00:04<00:02, 85.78it/s]\u001b[A\n",
            "Epoch 4:  74%|███████▍  | 474/640 [00:04<00:01, 86.79it/s]\u001b[A\n",
            "Epoch 4:  76%|███████▌  | 484/640 [00:04<00:01, 88.08it/s]\u001b[A\n",
            "Epoch 4:  77%|███████▋  | 493/640 [00:05<00:01, 88.54it/s]\u001b[A\n",
            "Epoch 4:  78%|███████▊  | 502/640 [00:05<00:01, 86.34it/s]\u001b[A\n",
            "Epoch 4:  80%|███████▉  | 511/640 [00:05<00:01, 87.24it/s]\u001b[A\n",
            "Epoch 4:  81%|████████▏ | 520/640 [00:05<00:01, 86.48it/s]\u001b[A\n",
            "Epoch 4:  83%|████████▎ | 530/640 [00:05<00:01, 88.21it/s]\u001b[A\n",
            "Epoch 4:  84%|████████▍ | 539/640 [00:05<00:01, 88.61it/s]\u001b[A\n",
            "Epoch 4:  86%|████████▌ | 548/640 [00:05<00:01, 87.98it/s]\u001b[A\n",
            "Epoch 4:  87%|████████▋ | 557/640 [00:05<00:00, 83.36it/s]\u001b[A\n",
            "Epoch 4:  88%|████████▊ | 566/640 [00:05<00:00, 81.31it/s]\u001b[A\n",
            "Epoch 4:  90%|████████▉ | 575/640 [00:05<00:00, 78.06it/s]\u001b[A\n",
            "Epoch 4:  91%|█████████ | 583/640 [00:06<00:00, 77.82it/s]\u001b[A\n",
            "Epoch 4:  92%|█████████▏| 591/640 [00:06<00:00, 76.67it/s]\u001b[A\n",
            "Epoch 4:  94%|█████████▎| 599/640 [00:06<00:00, 76.41it/s]\u001b[A\n",
            "Epoch 4:  95%|█████████▍| 607/640 [00:06<00:00, 77.06it/s]\u001b[A\n",
            "Epoch 4:  96%|█████████▌| 615/640 [00:06<00:00, 77.29it/s]\u001b[A\n",
            "Epoch 4:  97%|█████████▋| 623/640 [00:06<00:00, 75.98it/s]\u001b[A\n",
            "Epoch 4:  99%|█████████▊| 631/640 [00:06<00:00, 74.68it/s]\u001b[A\n",
            "Epoch 4: 100%|█████████▉| 639/640 [00:06<00:00, 74.18it/s]\u001b[A\n",
            "Epochs:   4%|▍         | 4/100 [00:27<11:07,  6.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4: Total rewards -34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 5:   0%|          | 0/640 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5:   1%|▏         | 8/640 [00:00<00:08, 78.59it/s]\u001b[A\n",
            "Epoch 5:   2%|▎         | 16/640 [00:00<00:08, 77.98it/s]\u001b[A\n",
            "Epoch 5:   4%|▍         | 24/640 [00:00<00:08, 74.98it/s]\u001b[A\n",
            "Epoch 5:   5%|▌         | 35/640 [00:00<00:06, 86.47it/s]\u001b[A\n",
            "Epoch 5:   7%|▋         | 47/640 [00:00<00:06, 97.82it/s]\u001b[A\n",
            "Epoch 5:   9%|▉         | 59/640 [00:00<00:05, 103.44it/s]\u001b[A\n",
            "Epoch 5:  11%|█         | 71/640 [00:00<00:05, 107.60it/s]\u001b[A\n",
            "Epoch 5:  13%|█▎        | 83/640 [00:00<00:05, 109.81it/s]\u001b[A\n",
            "Epoch 5:  15%|█▍        | 95/640 [00:00<00:04, 111.10it/s]\u001b[A\n",
            "Epoch 5:  17%|█▋        | 108/640 [00:01<00:04, 114.56it/s]\u001b[A\n",
            "Epoch 5:  19%|█▉        | 120/640 [00:01<00:04, 115.47it/s]\u001b[A\n",
            "Epoch 5:  21%|██        | 132/640 [00:01<00:04, 112.50it/s]\u001b[A\n",
            "Epoch 5:  22%|██▎       | 144/640 [00:01<00:04, 114.12it/s]\u001b[A\n",
            "Epoch 5:  24%|██▍       | 156/640 [00:01<00:04, 115.46it/s]\u001b[A\n",
            "Epoch 5:  26%|██▋       | 169/640 [00:01<00:03, 118.08it/s]\u001b[A\n",
            "Epoch 5:  28%|██▊       | 182/640 [00:01<00:03, 118.43it/s]\u001b[A\n",
            "Epoch 5:  30%|███       | 194/640 [00:01<00:03, 116.00it/s]\u001b[A\n",
            "Epoch 5:  32%|███▏      | 206/640 [00:01<00:03, 115.70it/s]\u001b[A\n",
            "Epoch 5:  34%|███▍      | 218/640 [00:02<00:03, 114.98it/s]\u001b[A\n",
            "Epoch 5:  36%|███▌      | 230/640 [00:02<00:03, 114.16it/s]\u001b[A\n",
            "Epoch 5:  38%|███▊      | 242/640 [00:02<00:03, 114.37it/s]\u001b[A\n",
            "Epoch 5:  40%|███▉      | 254/640 [00:02<00:03, 110.50it/s]\u001b[A\n",
            "Epoch 5:  42%|████▏     | 266/640 [00:02<00:03, 109.26it/s]\u001b[A\n",
            "Epoch 5:  43%|████▎     | 278/640 [00:02<00:03, 111.43it/s]\u001b[A\n",
            "Epoch 5:  45%|████▌     | 290/640 [00:02<00:03, 110.99it/s]\u001b[A\n",
            "Epoch 5:  47%|████▋     | 302/640 [00:02<00:03, 111.38it/s]\u001b[A\n",
            "Epoch 5:  49%|████▉     | 314/640 [00:02<00:02, 110.13it/s]\u001b[A\n",
            "Epoch 5:  51%|█████     | 326/640 [00:02<00:02, 111.03it/s]\u001b[A\n",
            "Epoch 5:  53%|█████▎    | 338/640 [00:03<00:02, 112.75it/s]\u001b[A\n",
            "Epoch 5:  55%|█████▍    | 350/640 [00:03<00:02, 114.55it/s]\u001b[A\n",
            "Epoch 5:  57%|█████▋    | 362/640 [00:03<00:02, 111.45it/s]\u001b[A\n",
            "Epoch 5:  58%|█████▊    | 374/640 [00:03<00:02, 109.72it/s]\u001b[A\n",
            "Epoch 5:  60%|██████    | 386/640 [00:03<00:02, 112.29it/s]\u001b[A\n",
            "Epoch 5:  62%|██████▏   | 399/640 [00:03<00:02, 114.85it/s]\u001b[A\n",
            "Epoch 5:  64%|██████▍   | 411/640 [00:03<00:01, 115.60it/s]\u001b[A\n",
            "Epoch 5:  66%|██████▌   | 423/640 [00:03<00:01, 115.59it/s]\u001b[A\n",
            "Epoch 5:  68%|██████▊   | 435/640 [00:03<00:01, 115.31it/s]\u001b[A\n",
            "Epoch 5:  70%|██████▉   | 447/640 [00:04<00:01, 115.36it/s]\u001b[A\n",
            "Epoch 5:  72%|███████▏  | 459/640 [00:04<00:01, 115.45it/s]\u001b[A\n",
            "Epoch 5:  74%|███████▎  | 471/640 [00:04<00:01, 115.55it/s]\u001b[A\n",
            "Epoch 5:  75%|███████▌  | 483/640 [00:04<00:01, 109.94it/s]\u001b[A\n",
            "Epoch 5:  77%|███████▋  | 495/640 [00:04<00:01, 112.34it/s]\u001b[A\n",
            "Epoch 5:  79%|███████▉  | 507/640 [00:04<00:01, 114.28it/s]\u001b[A\n",
            "Epoch 5:  81%|████████  | 519/640 [00:04<00:01, 115.29it/s]\u001b[A\n",
            "Epoch 5:  83%|████████▎ | 531/640 [00:04<00:00, 115.02it/s]\u001b[A\n",
            "Epoch 5:  85%|████████▍ | 543/640 [00:04<00:00, 109.10it/s]\u001b[A\n",
            "Epoch 5:  87%|████████▋ | 554/640 [00:04<00:00, 108.83it/s]\u001b[A\n",
            "Epoch 5:  88%|████████▊ | 566/640 [00:05<00:00, 111.06it/s]\u001b[A\n",
            "Epoch 5:  90%|█████████ | 578/640 [00:05<00:00, 113.61it/s]\u001b[A\n",
            "Epoch 5:  92%|█████████▏| 590/640 [00:05<00:00, 114.51it/s]\u001b[A\n",
            "Epoch 5:  94%|█████████▍| 602/640 [00:05<00:00, 110.79it/s]\u001b[A\n",
            "Epoch 5:  96%|█████████▌| 614/640 [00:05<00:00, 112.93it/s]\u001b[A\n",
            "Epoch 5:  98%|█████████▊| 627/640 [00:05<00:00, 115.22it/s]\u001b[A\n",
            "Epoch 5: 100%|█████████▉| 639/640 [00:05<00:00, 115.34it/s]\u001b[A\n",
            "Epochs:   5%|▌         | 5/100 [00:33<10:19,  6.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5: Total rewards 34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 6:   0%|          | 0/640 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6:   2%|▏         | 12/640 [00:00<00:05, 119.04it/s]\u001b[A\n",
            "Epoch 6:   4%|▍         | 24/640 [00:00<00:05, 112.87it/s]\u001b[A\n",
            "Epoch 6:   6%|▌         | 36/640 [00:00<00:05, 114.85it/s]\u001b[A\n",
            "Epoch 6:   8%|▊         | 48/640 [00:00<00:05, 115.43it/s]\u001b[A\n",
            "Epoch 6:   9%|▉         | 60/640 [00:00<00:05, 115.58it/s]\u001b[A\n",
            "Epoch 6:  11%|█▏        | 72/640 [00:00<00:05, 112.97it/s]\u001b[A\n",
            "Epoch 6:  13%|█▎        | 84/640 [00:00<00:04, 113.58it/s]\u001b[A\n",
            "Epoch 6:  15%|█▌        | 97/640 [00:00<00:04, 116.52it/s]\u001b[A\n",
            "Epoch 6:  17%|█▋        | 109/640 [00:00<00:04, 116.53it/s]\u001b[A\n",
            "Epoch 6:  19%|█▉        | 121/640 [00:01<00:04, 115.69it/s]\u001b[A\n",
            "Epoch 6:  21%|██        | 133/640 [00:01<00:04, 115.65it/s]\u001b[A\n",
            "Epoch 6:  23%|██▎       | 145/640 [00:01<00:04, 115.55it/s]\u001b[A\n",
            "Epoch 6:  25%|██▍       | 157/640 [00:01<00:04, 115.91it/s]\u001b[A\n",
            "Epoch 6:  26%|██▋       | 169/640 [00:01<00:04, 115.07it/s]\u001b[A\n",
            "Epoch 6:  28%|██▊       | 181/640 [00:01<00:04, 112.65it/s]\u001b[A\n",
            "Epoch 6:  30%|███       | 193/640 [00:01<00:04, 107.39it/s]\u001b[A\n",
            "Epoch 6:  32%|███▏      | 204/640 [00:01<00:04, 104.87it/s]\u001b[A\n",
            "Epoch 6:  34%|███▎      | 215/640 [00:01<00:03, 106.29it/s]\u001b[A\n",
            "Epoch 6:  35%|███▌      | 227/640 [00:02<00:03, 108.79it/s]\u001b[A\n",
            "Epoch 6:  37%|███▋      | 239/640 [00:02<00:03, 111.02it/s]\u001b[A\n",
            "Epoch 6:  39%|███▉      | 251/640 [00:02<00:03, 112.67it/s]\u001b[A\n",
            "Epoch 6:  41%|████      | 263/640 [00:02<00:03, 112.35it/s]\u001b[A\n",
            "Epoch 6:  43%|████▎     | 275/640 [00:02<00:03, 114.19it/s]\u001b[A\n",
            "Epoch 6:  45%|████▍     | 287/640 [00:02<00:03, 113.21it/s]\u001b[A\n",
            "Epoch 6:  47%|████▋     | 299/640 [00:02<00:03, 112.03it/s]\u001b[A\n",
            "Epoch 6:  49%|████▉     | 312/640 [00:02<00:02, 114.24it/s]\u001b[A\n",
            "Epoch 6:  51%|█████     | 325/640 [00:02<00:02, 116.28it/s]\u001b[A\n",
            "Epoch 6:  53%|█████▎    | 337/640 [00:02<00:02, 116.28it/s]\u001b[A\n",
            "Epoch 6:  55%|█████▍    | 349/640 [00:03<00:02, 117.15it/s]\u001b[A\n",
            "Epoch 6:  56%|█████▋    | 361/640 [00:03<00:02, 116.34it/s]\u001b[A\n",
            "Epoch 6:  58%|█████▊    | 373/640 [00:03<00:02, 117.32it/s]\u001b[A\n",
            "Epoch 6:  60%|██████    | 386/640 [00:03<00:02, 118.50it/s]\u001b[A\n",
            "Epoch 6:  62%|██████▏   | 398/640 [00:03<00:02, 116.63it/s]\u001b[A\n",
            "Epoch 6:  64%|██████▍   | 410/640 [00:03<00:01, 116.51it/s]\u001b[A\n",
            "Epoch 6:  66%|██████▌   | 422/640 [00:03<00:01, 114.78it/s]\u001b[A\n",
            "Epoch 6:  68%|██████▊   | 434/640 [00:03<00:01, 113.18it/s]\u001b[A\n",
            "Epoch 6:  70%|██████▉   | 447/640 [00:03<00:01, 115.61it/s]\u001b[A\n",
            "Epoch 6:  72%|███████▏  | 459/640 [00:04<00:01, 116.51it/s]\u001b[A\n",
            "Epoch 6:  74%|███████▎  | 471/640 [00:04<00:01, 116.85it/s]\u001b[A\n",
            "Epoch 6:  75%|███████▌  | 483/640 [00:04<00:01, 116.84it/s]\u001b[A\n",
            "Epoch 6:  77%|███████▋  | 495/640 [00:04<00:01, 113.88it/s]\u001b[A\n",
            "Epoch 6:  79%|███████▉  | 507/640 [00:04<00:01, 114.00it/s]\u001b[A\n",
            "Epoch 6:  81%|████████  | 519/640 [00:04<00:01, 113.18it/s]\u001b[A\n",
            "Epoch 6:  83%|████████▎ | 531/640 [00:04<00:01, 101.58it/s]\u001b[A\n",
            "Epoch 6:  85%|████████▍ | 542/640 [00:04<00:01, 96.56it/s] \u001b[A\n",
            "Epoch 6:  86%|████████▋ | 552/640 [00:04<00:00, 92.86it/s]\u001b[A\n",
            "Epoch 6:  88%|████████▊ | 562/640 [00:05<00:00, 92.24it/s]\u001b[A\n",
            "Epoch 6:  89%|████████▉ | 572/640 [00:05<00:00, 91.62it/s]\u001b[A\n",
            "Epoch 6:  91%|█████████ | 582/640 [00:05<00:00, 91.25it/s]\u001b[A\n",
            "Epoch 6:  92%|█████████▎| 592/640 [00:05<00:00, 91.61it/s]\u001b[A\n",
            "Epoch 6:  94%|█████████▍| 602/640 [00:05<00:00, 89.45it/s]\u001b[A\n",
            "Epoch 6:  96%|█████████▌| 612/640 [00:05<00:00, 90.18it/s]\u001b[A\n",
            "Epoch 6:  97%|█████████▋| 622/640 [00:05<00:00, 87.64it/s]\u001b[A\n",
            "Epoch 6:  99%|█████████▊| 631/640 [00:05<00:00, 87.85it/s]\u001b[A\n",
            "Epochs:   6%|▌         | 6/100 [00:39<09:54,  6.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6: Total rewards -28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 7:   0%|          | 0/640 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7:   1%|▏         | 9/640 [00:00<00:07, 89.71it/s]\u001b[A\n",
            "Epoch 7:   3%|▎         | 18/640 [00:00<00:07, 86.21it/s]\u001b[A\n",
            "Epoch 7:   4%|▍         | 27/640 [00:00<00:06, 87.72it/s]\u001b[A\n",
            "Epoch 7:   6%|▌         | 37/640 [00:00<00:06, 89.15it/s]\u001b[A\n",
            "Epoch 7:   7%|▋         | 47/640 [00:00<00:06, 91.05it/s]\u001b[A\n",
            "Epoch 7:   9%|▉         | 57/640 [00:00<00:06, 90.14it/s]\u001b[A\n",
            "Epoch 7:  10%|█         | 67/640 [00:00<00:06, 91.02it/s]\u001b[A\n",
            "Epoch 7:  12%|█▏        | 77/640 [00:00<00:06, 89.79it/s]\u001b[A\n",
            "Epoch 7:  14%|█▎        | 87/640 [00:00<00:06, 90.13it/s]\u001b[A\n",
            "Epoch 7:  15%|█▌        | 97/640 [00:01<00:06, 89.02it/s]\u001b[A\n",
            "Epoch 7:  17%|█▋        | 106/640 [00:01<00:06, 84.44it/s]\u001b[A\n",
            "Epoch 7:  18%|█▊        | 115/640 [00:01<00:06, 82.36it/s]\u001b[A\n",
            "Epoch 7:  19%|█▉        | 124/640 [00:01<00:06, 82.04it/s]\u001b[A\n",
            "Epoch 7:  21%|██        | 133/640 [00:01<00:06, 83.70it/s]\u001b[A\n",
            "Epoch 7:  22%|██▏       | 142/640 [00:01<00:05, 85.42it/s]\u001b[A\n",
            "Epoch 7:  24%|██▍       | 152/640 [00:01<00:05, 87.40it/s]\u001b[A\n",
            "Epoch 7:  25%|██▌       | 161/640 [00:01<00:05, 87.53it/s]\u001b[A\n",
            "Epoch 7:  27%|██▋       | 170/640 [00:01<00:05, 86.68it/s]\u001b[A\n",
            "Epoch 7:  28%|██▊       | 179/640 [00:02<00:05, 86.57it/s]\u001b[A\n",
            "Epoch 7:  29%|██▉       | 188/640 [00:02<00:05, 87.09it/s]\u001b[A\n",
            "Epoch 7:  31%|███       | 197/640 [00:02<00:05, 85.70it/s]\u001b[A\n",
            "Epoch 7:  32%|███▏      | 206/640 [00:02<00:05, 83.92it/s]\u001b[A\n",
            "Epoch 7:  34%|███▎      | 215/640 [00:02<00:05, 82.47it/s]\u001b[A\n",
            "Epoch 7:  35%|███▌      | 224/640 [00:02<00:05, 80.79it/s]\u001b[A\n",
            "Epoch 7:  36%|███▋      | 233/640 [00:02<00:05, 80.63it/s]\u001b[A\n",
            "Epoch 7:  38%|███▊      | 242/640 [00:02<00:05, 79.26it/s]\u001b[A\n",
            "Epoch 7:  39%|███▉      | 250/640 [00:02<00:05, 76.69it/s]\u001b[A\n",
            "Epoch 7:  40%|████      | 258/640 [00:03<00:04, 76.96it/s]\u001b[A\n",
            "Epoch 7:  42%|████▏     | 266/640 [00:03<00:04, 77.64it/s]\u001b[A\n",
            "Epoch 7:  43%|████▎     | 274/640 [00:03<00:04, 76.53it/s]\u001b[A\n",
            "Epoch 7:  44%|████▍     | 282/640 [00:03<00:04, 76.14it/s]\u001b[A\n",
            "Epoch 7:  45%|████▌     | 290/640 [00:03<00:04, 74.73it/s]\u001b[A\n",
            "Epoch 7:  47%|████▋     | 298/640 [00:03<00:04, 76.11it/s]\u001b[A\n",
            "Epoch 7:  48%|████▊     | 306/640 [00:03<00:04, 75.96it/s]\u001b[A\n",
            "Epoch 7:  49%|████▉     | 315/640 [00:03<00:04, 77.42it/s]\u001b[A\n",
            "Epoch 7:  51%|█████     | 325/640 [00:03<00:03, 82.45it/s]\u001b[A\n",
            "Epoch 7:  53%|█████▎    | 337/640 [00:04<00:03, 92.09it/s]\u001b[A\n",
            "Epoch 7:  55%|█████▍    | 349/640 [00:04<00:02, 99.32it/s]\u001b[A\n",
            "Epoch 7:  57%|█████▋    | 362/640 [00:04<00:02, 106.05it/s]\u001b[A\n",
            "Epoch 7:  59%|█████▊    | 375/640 [00:04<00:02, 110.72it/s]\u001b[A\n",
            "Epoch 7:  61%|██████    | 388/640 [00:04<00:02, 114.06it/s]\u001b[A\n",
            "Epoch 7:  62%|██████▎   | 400/640 [00:04<00:02, 114.41it/s]\u001b[A\n",
            "Epoch 7:  64%|██████▍   | 412/640 [00:04<00:01, 115.59it/s]\u001b[A\n",
            "Epoch 7:  66%|██████▋   | 424/640 [00:04<00:01, 116.12it/s]\u001b[A\n",
            "Epoch 7:  68%|██████▊   | 436/640 [00:04<00:01, 115.38it/s]\u001b[A\n",
            "Epoch 7:  70%|███████   | 448/640 [00:04<00:01, 113.28it/s]\u001b[A\n",
            "Epoch 7:  72%|███████▏  | 460/640 [00:05<00:01, 112.85it/s]\u001b[A\n",
            "Epoch 7:  74%|███████▍  | 472/640 [00:05<00:01, 113.80it/s]\u001b[A\n",
            "Epoch 7:  76%|███████▌  | 485/640 [00:05<00:01, 116.15it/s]\u001b[A\n",
            "Epoch 7:  78%|███████▊  | 497/640 [00:05<00:01, 116.84it/s]\u001b[A\n",
            "Epoch 7:  80%|███████▉  | 509/640 [00:05<00:01, 117.05it/s]\u001b[A\n",
            "Epoch 7:  81%|████████▏ | 521/640 [00:05<00:01, 112.75it/s]\u001b[A\n",
            "Epoch 7:  83%|████████▎ | 533/640 [00:05<00:00, 112.62it/s]\u001b[A\n",
            "Epoch 7:  85%|████████▌ | 545/640 [00:05<00:00, 108.50it/s]\u001b[A\n",
            "Epoch 7:  87%|████████▋ | 556/640 [00:05<00:00, 108.69it/s]\u001b[A\n",
            "Epoch 7:  89%|████████▊ | 567/640 [00:06<00:00, 108.45it/s]\u001b[A\n",
            "Epoch 7:  90%|█████████ | 579/640 [00:06<00:00, 110.10it/s]\u001b[A\n",
            "Epoch 7:  92%|█████████▏| 591/640 [00:06<00:00, 112.87it/s]\u001b[A\n",
            "Epoch 7:  94%|█████████▍| 604/640 [00:06<00:00, 115.59it/s]\u001b[A\n",
            "Epoch 7:  96%|█████████▋| 616/640 [00:06<00:00, 116.63it/s]\u001b[A\n",
            "Epoch 7:  98%|█████████▊| 628/640 [00:06<00:00, 117.57it/s]\u001b[A\n",
            "Epoch 7: 100%|██████████| 640/640 [00:06<00:00, 114.72it/s]\u001b[A\n",
            "Epochs:   7%|▋         | 7/100 [00:45<09:59,  6.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7: Total rewards 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 8:   0%|          | 0/640 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8:   2%|▏         | 12/640 [00:00<00:05, 118.30it/s]\u001b[A\n",
            "Epoch 8:   4%|▍         | 24/640 [00:00<00:05, 117.60it/s]\u001b[A\n",
            "Epoch 8:   6%|▌         | 36/640 [00:00<00:05, 116.39it/s]\u001b[A\n",
            "Epoch 8:   8%|▊         | 48/640 [00:00<00:05, 114.11it/s]\u001b[A\n",
            "Epoch 8:   9%|▉         | 60/640 [00:00<00:05, 113.36it/s]\u001b[A\n",
            "Epoch 8:  11%|█▏        | 72/640 [00:00<00:04, 114.77it/s]\u001b[A\n",
            "Epoch 8:  13%|█▎        | 84/640 [00:00<00:04, 114.60it/s]\u001b[A\n",
            "Epoch 8:  15%|█▌        | 97/640 [00:00<00:04, 117.14it/s]\u001b[A\n",
            "Epoch 8:  17%|█▋        | 109/640 [00:00<00:04, 116.69it/s]\u001b[A\n",
            "Epoch 8:  19%|█▉        | 121/640 [00:01<00:04, 115.84it/s]\u001b[A\n",
            "Epoch 8:  21%|██        | 134/640 [00:01<00:04, 117.69it/s]\u001b[A\n",
            "Epoch 8:  23%|██▎       | 147/640 [00:01<00:04, 118.98it/s]\u001b[A\n",
            "Epoch 8:  25%|██▍       | 159/640 [00:01<00:04, 115.22it/s]\u001b[A\n",
            "Epoch 8:  27%|██▋       | 171/640 [00:01<00:04, 112.53it/s]\u001b[A\n",
            "Epoch 8:  29%|██▊       | 183/640 [00:01<00:04, 114.02it/s]\u001b[A\n",
            "Epoch 8:  30%|███       | 195/640 [00:01<00:03, 114.25it/s]\u001b[A\n",
            "Epoch 8:  32%|███▏      | 207/640 [00:01<00:03, 115.23it/s]\u001b[A\n",
            "Epoch 8:  34%|███▍      | 219/640 [00:01<00:03, 115.33it/s]\u001b[A\n",
            "Epoch 8:  36%|███▌      | 231/640 [00:02<00:03, 113.67it/s]\u001b[A\n",
            "Epoch 8:  38%|███▊      | 243/640 [00:02<00:03, 112.04it/s]\u001b[A\n",
            "Epoch 8:  40%|███▉      | 255/640 [00:02<00:03, 112.50it/s]\u001b[A\n",
            "Epoch 8:  42%|████▏     | 267/640 [00:02<00:03, 112.91it/s]\u001b[A\n",
            "Epoch 8:  44%|████▎     | 279/640 [00:02<00:03, 111.74it/s]\u001b[A\n",
            "Epoch 8:  45%|████▌     | 291/640 [00:02<00:03, 112.17it/s]\u001b[A\n",
            "Epoch 8:  47%|████▋     | 303/640 [00:02<00:03, 112.17it/s]\u001b[A\n",
            "Epoch 8:  49%|████▉     | 315/640 [00:02<00:02, 112.96it/s]\u001b[A\n",
            "Epoch 8:  51%|█████     | 327/640 [00:02<00:02, 114.70it/s]\u001b[A\n",
            "Epoch 8:  53%|█████▎    | 339/640 [00:02<00:02, 115.98it/s]\u001b[A\n",
            "Epoch 8:  55%|█████▍    | 351/640 [00:03<00:02, 114.87it/s]\u001b[A\n",
            "Epoch 8:  57%|█████▋    | 363/640 [00:03<00:02, 115.50it/s]\u001b[A\n",
            "Epoch 8:  59%|█████▊    | 375/640 [00:03<00:02, 116.66it/s]\u001b[A\n",
            "Epoch 8:  60%|██████    | 387/640 [00:03<00:02, 116.59it/s]\u001b[A\n",
            "Epoch 8:  62%|██████▏   | 399/640 [00:03<00:02, 115.47it/s]\u001b[A\n",
            "Epoch 8:  64%|██████▍   | 411/640 [00:03<00:01, 115.85it/s]\u001b[A\n",
            "Epoch 8:  66%|██████▋   | 424/640 [00:03<00:01, 117.68it/s]\u001b[A\n",
            "Epoch 8:  68%|██████▊   | 436/640 [00:03<00:01, 117.93it/s]\u001b[A\n",
            "Epoch 8:  70%|███████   | 449/640 [00:03<00:01, 118.75it/s]\u001b[A\n",
            "Epoch 8:  72%|███████▏  | 461/640 [00:03<00:01, 117.53it/s]\u001b[A\n",
            "Epoch 8:  74%|███████▍  | 473/640 [00:04<00:01, 116.11it/s]\u001b[A\n",
            "Epoch 8:  76%|███████▌  | 486/640 [00:04<00:01, 117.61it/s]\u001b[A\n",
            "Epoch 8:  78%|███████▊  | 498/640 [00:04<00:01, 117.59it/s]\u001b[A\n",
            "Epoch 8:  80%|███████▉  | 510/640 [00:04<00:01, 114.74it/s]\u001b[A\n",
            "Epoch 8:  82%|████████▏ | 522/640 [00:04<00:01, 115.41it/s]\u001b[A\n",
            "Epoch 8:  83%|████████▎ | 534/640 [00:04<00:00, 114.24it/s]\u001b[A\n",
            "Epoch 8:  85%|████████▌ | 546/640 [00:04<00:00, 114.97it/s]\u001b[A\n",
            "Epoch 8:  87%|████████▋ | 558/640 [00:04<00:00, 116.01it/s]\u001b[A\n",
            "Epoch 8:  89%|████████▉ | 570/640 [00:04<00:00, 115.24it/s]\u001b[A\n",
            "Epoch 8:  91%|█████████ | 582/640 [00:05<00:00, 116.10it/s]\u001b[A\n",
            "Epoch 8:  93%|█████████▎| 594/640 [00:05<00:00, 116.03it/s]\u001b[A\n",
            "Epoch 8:  95%|█████████▍| 607/640 [00:05<00:00, 117.74it/s]\u001b[A\n",
            "Epoch 8:  97%|█████████▋| 619/640 [00:05<00:00, 116.62it/s]\u001b[A\n",
            "Epoch 8:  99%|█████████▊| 631/640 [00:05<00:00, 115.00it/s]\u001b[A\n",
            "Epochs:   8%|▊         | 8/100 [00:51<09:26,  6.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8: Total rewards 72\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 9:   0%|          | 0/640 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9:   2%|▏         | 12/640 [00:00<00:05, 116.69it/s]\u001b[A\n",
            "Epoch 9:   4%|▍         | 24/640 [00:00<00:05, 116.37it/s]\u001b[A\n",
            "Epoch 9:   6%|▌         | 36/640 [00:00<00:05, 114.90it/s]\u001b[A\n",
            "Epoch 9:   8%|▊         | 48/640 [00:00<00:05, 114.11it/s]\u001b[A\n",
            "Epoch 9:   9%|▉         | 60/640 [00:00<00:05, 115.98it/s]\u001b[A\n",
            "Epoch 9:  11%|█▏        | 72/640 [00:00<00:04, 114.27it/s]\u001b[A\n",
            "Epoch 9:  13%|█▎        | 84/640 [00:00<00:04, 114.47it/s]\u001b[A\n",
            "Epoch 9:  15%|█▌        | 96/640 [00:00<00:04, 114.37it/s]\u001b[A\n",
            "Epoch 9:  17%|█▋        | 108/640 [00:00<00:04, 110.65it/s]\u001b[A\n",
            "Epoch 9:  19%|█▉        | 120/640 [00:01<00:04, 112.40it/s]\u001b[A\n",
            "Epoch 9:  21%|██        | 132/640 [00:01<00:04, 113.99it/s]\u001b[A\n",
            "Epoch 9:  23%|██▎       | 145/640 [00:01<00:04, 116.49it/s]\u001b[A\n",
            "Epoch 9:  25%|██▍       | 157/640 [00:01<00:04, 114.95it/s]\u001b[A\n",
            "Epoch 9:  26%|██▋       | 169/640 [00:01<00:04, 114.63it/s]\u001b[A\n",
            "Epoch 9:  28%|██▊       | 181/640 [00:01<00:04, 114.07it/s]\u001b[A\n",
            "Epoch 9:  30%|███       | 193/640 [00:01<00:04, 106.89it/s]\u001b[A\n",
            "Epoch 9:  32%|███▏      | 204/640 [00:01<00:04, 98.22it/s] \u001b[A\n",
            "Epoch 9:  33%|███▎      | 214/640 [00:01<00:04, 91.64it/s]\u001b[A\n",
            "Epoch 9:  35%|███▌      | 224/640 [00:02<00:04, 90.92it/s]\u001b[A\n",
            "Epoch 9:  37%|███▋      | 234/640 [00:02<00:04, 89.83it/s]\u001b[A\n",
            "Epoch 9:  38%|███▊      | 244/640 [00:02<00:04, 90.09it/s]\u001b[A\n",
            "Epoch 9:  40%|███▉      | 254/640 [00:02<00:04, 89.84it/s]\u001b[A\n",
            "Epoch 9:  41%|████▏     | 264/640 [00:02<00:04, 90.46it/s]\u001b[A\n",
            "Epoch 9:  43%|████▎     | 274/640 [00:02<00:04, 89.55it/s]\u001b[A\n",
            "Epoch 9:  44%|████▍     | 283/640 [00:02<00:03, 89.30it/s]\u001b[A\n",
            "Epoch 9:  46%|████▌     | 292/640 [00:02<00:03, 88.97it/s]\u001b[A\n",
            "Epoch 9:  47%|████▋     | 301/640 [00:02<00:03, 88.92it/s]\u001b[A\n",
            "Epoch 9:  48%|████▊     | 310/640 [00:03<00:03, 88.61it/s]\u001b[A\n",
            "Epoch 9:  50%|████▉     | 319/640 [00:03<00:03, 88.00it/s]\u001b[A\n",
            "Epoch 9:  51%|█████▏    | 328/640 [00:03<00:03, 86.95it/s]\u001b[A\n",
            "Epoch 9:  53%|█████▎    | 337/640 [00:03<00:03, 87.43it/s]\u001b[A\n",
            "Epoch 9:  54%|█████▍    | 346/640 [00:03<00:03, 87.72it/s]\u001b[A\n",
            "Epoch 9:  56%|█████▌    | 356/640 [00:03<00:03, 89.37it/s]\u001b[A\n",
            "Epoch 9:  57%|█████▋    | 366/640 [00:03<00:03, 90.78it/s]\u001b[A\n",
            "Epoch 9:  59%|█████▉    | 376/640 [00:03<00:02, 91.44it/s]\u001b[A\n",
            "Epoch 9:  60%|██████    | 386/640 [00:03<00:02, 92.52it/s]\u001b[A\n",
            "Epoch 9:  62%|██████▏   | 396/640 [00:04<00:02, 90.19it/s]\u001b[A\n",
            "Epoch 9:  63%|██████▎   | 406/640 [00:04<00:02, 90.47it/s]\u001b[A\n",
            "Epoch 9:  65%|██████▌   | 416/640 [00:04<00:02, 87.58it/s]\u001b[A\n",
            "Epoch 9:  66%|██████▋   | 425/640 [00:04<00:02, 87.19it/s]\u001b[A\n",
            "Epoch 9:  68%|██████▊   | 434/640 [00:04<00:02, 87.03it/s]\u001b[A\n",
            "Epoch 9:  69%|██████▉   | 444/640 [00:04<00:02, 88.41it/s]\u001b[A\n",
            "Epoch 9:  71%|███████   | 454/640 [00:04<00:02, 89.98it/s]\u001b[A\n",
            "Epoch 9:  72%|███████▎  | 464/640 [00:04<00:01, 89.44it/s]\u001b[A\n",
            "Epoch 9:  74%|███████▍  | 474/640 [00:04<00:01, 89.66it/s]\u001b[A\n",
            "Epoch 9:  75%|███████▌  | 483/640 [00:05<00:01, 87.92it/s]\u001b[A\n",
            "Epoch 9:  77%|███████▋  | 492/640 [00:05<00:01, 85.72it/s]\u001b[A\n",
            "Epoch 9:  78%|███████▊  | 501/640 [00:05<00:01, 85.92it/s]\u001b[A\n",
            "Epoch 9:  80%|███████▉  | 510/640 [00:05<00:01, 85.79it/s]\u001b[A\n",
            "Epoch 9:  81%|████████  | 519/640 [00:05<00:01, 83.40it/s]\u001b[A\n",
            "Epoch 9:  82%|████████▎ | 528/640 [00:05<00:01, 81.50it/s]\u001b[A\n",
            "Epoch 9:  84%|████████▍ | 537/640 [00:05<00:01, 80.68it/s]\u001b[A\n",
            "Epoch 9:  85%|████████▌ | 546/640 [00:05<00:01, 78.94it/s]\u001b[A\n",
            "Epoch 9:  87%|████████▋ | 554/640 [00:05<00:01, 78.43it/s]\u001b[A\n",
            "Epoch 9:  88%|████████▊ | 562/640 [00:05<00:01, 77.88it/s]\u001b[A\n",
            "Epoch 9:  89%|████████▉ | 570/640 [00:06<00:00, 75.44it/s]\u001b[A\n",
            "Epoch 9:  90%|█████████ | 578/640 [00:06<00:00, 75.42it/s]\u001b[A\n",
            "Epoch 9:  92%|█████████▏| 586/640 [00:06<00:00, 74.63it/s]\u001b[A\n",
            "Epoch 9:  93%|█████████▎| 594/640 [00:06<00:00, 73.94it/s]\u001b[A\n",
            "Epoch 9:  94%|█████████▍| 602/640 [00:06<00:00, 74.50it/s]\u001b[A\n",
            "Epoch 9:  95%|█████████▌| 610/640 [00:06<00:00, 74.90it/s]\u001b[A\n",
            "Epoch 9:  97%|█████████▋| 618/640 [00:06<00:00, 75.84it/s]\u001b[A\n",
            "Epoch 9:  98%|█████████▊| 626/640 [00:06<00:00, 76.21it/s]\u001b[A\n",
            "Epoch 9:  99%|█████████▉| 634/640 [00:06<00:00, 74.09it/s]\u001b[A\n",
            "Epochs:   9%|▉         | 9/100 [00:58<09:46,  6.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9: Total rewards 28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 10:   0%|          | 0/640 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10:   2%|▏         | 12/640 [00:00<00:05, 112.44it/s]\u001b[A\n",
            "Epoch 10:   4%|▍         | 24/640 [00:00<00:05, 111.45it/s]\u001b[A\n",
            "Epoch 10:   6%|▌         | 36/640 [00:00<00:05, 110.67it/s]\u001b[A\n",
            "Epoch 10:   8%|▊         | 48/640 [00:00<00:05, 110.28it/s]\u001b[A\n",
            "Epoch 10:   9%|▉         | 60/640 [00:00<00:05, 112.42it/s]\u001b[A\n",
            "Epoch 10:  11%|█▏        | 72/640 [00:00<00:05, 113.29it/s]\u001b[A\n",
            "Epoch 10:  13%|█▎        | 84/640 [00:00<00:04, 111.46it/s]\u001b[A\n",
            "Epoch 10:  15%|█▌        | 96/640 [00:00<00:04, 113.65it/s]\u001b[A\n",
            "Epoch 10:  17%|█▋        | 108/640 [00:00<00:04, 108.40it/s]\u001b[A\n",
            "Epoch 10:  19%|█▊        | 119/640 [00:01<00:04, 106.52it/s]\u001b[A\n",
            "Epoch 10:  20%|██        | 131/640 [00:01<00:04, 107.95it/s]\u001b[A\n",
            "Epoch 10:  22%|██▏       | 143/640 [00:01<00:04, 108.88it/s]\u001b[A\n",
            "Epoch 10:  24%|██▍       | 155/640 [00:01<00:04, 109.56it/s]\u001b[A\n",
            "Epoch 10:  26%|██▌       | 166/640 [00:01<00:04, 108.42it/s]\u001b[A\n",
            "Epoch 10:  28%|██▊       | 177/640 [00:01<00:04, 108.37it/s]\u001b[A\n",
            "Epoch 10:  30%|██▉       | 189/640 [00:01<00:04, 109.98it/s]\u001b[A\n",
            "Epoch 10:  31%|███▏      | 201/640 [00:01<00:03, 111.11it/s]\u001b[A\n",
            "Epoch 10:  33%|███▎      | 213/640 [00:01<00:03, 112.84it/s]\u001b[A\n",
            "Epoch 10:  35%|███▌      | 225/640 [00:02<00:03, 112.98it/s]\u001b[A\n",
            "Epoch 10:  37%|███▋      | 237/640 [00:02<00:03, 112.45it/s]\u001b[A\n",
            "Epoch 10:  39%|███▉      | 249/640 [00:02<00:03, 113.59it/s]\u001b[A\n",
            "Epoch 10:  41%|████      | 261/640 [00:02<00:03, 109.97it/s]\u001b[A\n",
            "Epoch 10:  43%|████▎     | 273/640 [00:02<00:03, 108.24it/s]\u001b[A\n",
            "Epoch 10:  45%|████▍     | 285/640 [00:02<00:03, 110.23it/s]\u001b[A\n",
            "Epoch 10:  46%|████▋     | 297/640 [00:02<00:03, 111.65it/s]\u001b[A\n",
            "Epoch 10:  48%|████▊     | 309/640 [00:02<00:02, 111.88it/s]\u001b[A\n",
            "Epoch 10:  50%|█████     | 321/640 [00:02<00:02, 111.93it/s]\u001b[A\n",
            "Epoch 10:  52%|█████▏    | 333/640 [00:03<00:02, 112.36it/s]\u001b[A\n",
            "Epoch 10:  54%|█████▍    | 345/640 [00:03<00:02, 111.38it/s]\u001b[A\n",
            "Epoch 10:  56%|█████▌    | 357/640 [00:03<00:02, 113.43it/s]\u001b[A\n",
            "Epoch 10:  58%|█████▊    | 369/640 [00:03<00:02, 112.55it/s]\u001b[A\n",
            "Epoch 10:  60%|█████▉    | 381/640 [00:03<00:02, 112.30it/s]\u001b[A\n",
            "Epoch 10:  61%|██████▏   | 393/640 [00:03<00:02, 113.52it/s]\u001b[A\n",
            "Epoch 10:  63%|██████▎   | 405/640 [00:03<00:02, 113.16it/s]\u001b[A\n",
            "Epoch 10:  65%|██████▌   | 417/640 [00:03<00:01, 114.44it/s]\u001b[A\n",
            "Epoch 10:  67%|██████▋   | 429/640 [00:03<00:01, 114.63it/s]\u001b[A\n",
            "Epoch 10:  69%|██████▉   | 441/640 [00:03<00:01, 114.98it/s]\u001b[A\n",
            "Epoch 10:  71%|███████   | 453/640 [00:04<00:01, 115.58it/s]\u001b[A\n",
            "Epoch 10:  73%|███████▎  | 465/640 [00:04<00:01, 113.82it/s]\u001b[A\n",
            "Epoch 10:  75%|███████▍  | 477/640 [00:04<00:01, 115.57it/s]\u001b[A\n",
            "Epoch 10:  76%|███████▋  | 489/640 [00:04<00:01, 115.81it/s]\u001b[A\n",
            "Epoch 10:  78%|███████▊  | 501/640 [00:04<00:01, 113.62it/s]\u001b[A\n",
            "Epoch 10:  80%|████████  | 513/640 [00:04<00:01, 114.57it/s]\u001b[A\n",
            "Epoch 10:  82%|████████▏ | 525/640 [00:04<00:00, 115.62it/s]\u001b[A\n",
            "Epoch 10:  84%|████████▍ | 537/640 [00:04<00:00, 116.35it/s]\u001b[A\n",
            "Epoch 10:  86%|████████▌ | 549/640 [00:04<00:00, 116.85it/s]\u001b[A\n",
            "Epoch 10:  88%|████████▊ | 561/640 [00:04<00:00, 116.40it/s]\u001b[A\n",
            "Epoch 10:  90%|████████▉ | 573/640 [00:05<00:00, 115.87it/s]\u001b[A\n",
            "Epoch 10:  91%|█████████▏| 585/640 [00:05<00:00, 114.36it/s]\u001b[A\n",
            "Epoch 10:  93%|█████████▎| 597/640 [00:05<00:00, 114.16it/s]\u001b[A\n",
            "Epoch 10:  95%|█████████▌| 609/640 [00:05<00:00, 114.26it/s]\u001b[A\n",
            "Epoch 10:  97%|█████████▋| 621/640 [00:05<00:00, 115.14it/s]\u001b[A\n",
            "Epoch 10:  99%|█████████▉| 633/640 [00:05<00:00, 113.36it/s]\u001b[A\n",
            "Epochs:  10%|█         | 10/100 [01:04<09:19,  6.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10: Total rewards 34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 11:   0%|          | 0/640 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11:   2%|▏         | 12/640 [00:00<00:05, 113.17it/s]\u001b[A\n",
            "Epoch 11:   4%|▍         | 24/640 [00:00<00:05, 109.45it/s]\u001b[A\n",
            "Epoch 11:   5%|▌         | 35/640 [00:00<00:05, 109.41it/s]\u001b[A\n",
            "Epoch 11:   7%|▋         | 47/640 [00:00<00:05, 110.53it/s]\u001b[A\n",
            "Epoch 11:   9%|▉         | 59/640 [00:00<00:05, 110.28it/s]\u001b[A\n",
            "Epoch 11:  11%|█         | 71/640 [00:00<00:05, 111.61it/s]\u001b[A\n",
            "Epoch 11:  13%|█▎        | 83/640 [00:00<00:04, 113.14it/s]\u001b[A\n",
            "Epoch 11:  15%|█▍        | 95/640 [00:00<00:04, 111.48it/s]\u001b[A\n",
            "Epoch 11:  17%|█▋        | 107/640 [00:00<00:04, 108.54it/s]\u001b[A\n",
            "Epoch 11:  18%|█▊        | 118/640 [00:01<00:04, 107.82it/s]\u001b[A\n",
            "Epoch 11:  20%|██        | 129/640 [00:01<00:04, 107.36it/s]\u001b[A\n",
            "Epoch 11:  22%|██▏       | 140/640 [00:01<00:04, 106.14it/s]\u001b[A\n",
            "Epoch 11:  24%|██▎       | 151/640 [00:01<00:04, 104.75it/s]\u001b[A\n",
            "Epoch 11:  25%|██▌       | 162/640 [00:01<00:04, 101.87it/s]\u001b[A\n",
            "Epoch 11:  27%|██▋       | 173/640 [00:01<00:04, 103.38it/s]\u001b[A\n",
            "Epoch 11:  29%|██▉       | 184/640 [00:01<00:04, 104.33it/s]\u001b[A\n",
            "Epoch 11:  30%|███       | 195/640 [00:01<00:04, 105.17it/s]\u001b[A\n",
            "Epoch 11:  32%|███▏      | 207/640 [00:01<00:04, 107.42it/s]\u001b[A\n",
            "Epoch 11:  34%|███▍      | 219/640 [00:02<00:03, 108.54it/s]\u001b[A\n",
            "Epoch 11:  36%|███▌      | 231/640 [00:02<00:03, 110.23it/s]\u001b[A\n",
            "Epoch 11:  38%|███▊      | 243/640 [00:02<00:03, 108.84it/s]\u001b[A\n",
            "Epoch 11:  40%|███▉      | 254/640 [00:02<00:03, 108.50it/s]\u001b[A\n",
            "Epoch 11:  42%|████▏     | 266/640 [00:02<00:03, 110.85it/s]\u001b[A\n",
            "Epoch 11:  43%|████▎     | 278/640 [00:02<00:03, 110.87it/s]\u001b[A\n",
            "Epoch 11:  45%|████▌     | 290/640 [00:02<00:03, 112.11it/s]\u001b[A\n",
            "Epoch 11:  47%|████▋     | 302/640 [00:02<00:02, 113.07it/s]\u001b[A\n",
            "Epoch 11:  49%|████▉     | 314/640 [00:02<00:02, 113.30it/s]\u001b[A\n",
            "Epoch 11:  51%|█████     | 326/640 [00:02<00:02, 114.54it/s]\u001b[A\n",
            "Epoch 11:  53%|█████▎    | 338/640 [00:03<00:02, 115.71it/s]\u001b[A\n",
            "Epoch 11:  55%|█████▍    | 350/640 [00:03<00:02, 116.85it/s]\u001b[A\n",
            "Epoch 11:  57%|█████▋    | 362/640 [00:03<00:02, 116.30it/s]\u001b[A\n",
            "Epoch 11:  58%|█████▊    | 374/640 [00:03<00:02, 115.28it/s]\u001b[A\n",
            "Epoch 11:  60%|██████    | 387/640 [00:03<00:02, 117.17it/s]\u001b[A\n",
            "Epochs:  10%|█         | 10/100 [01:07<10:09,  6.77s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-236d01b37c0c>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0;31m#softmax for model output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m         distilbert_output = self.distilbert(\n\u001b[0m\u001b[1;32m    764\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m         return self.transformer(\n\u001b[0m\u001b[1;32m    584\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    357\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    360\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;31m# Feed Forward Network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0mffn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0mffn_output\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mffn_output\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msa_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mapply_chunking_to_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mff_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mff_chunk\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mff_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "import cufflinks as cf\n",
        "import pandas as pd\n",
        "\n",
        "#assuming all_rewards is a list of lists where each sublist is rewards of one fold\n",
        "all_rewards_df = pd.DataFrame(all_rewards).T #transpose to have each fold as a column\n",
        "\n",
        "moving_avg_rewards = all_rewards_df.rolling(window=10).mean()\n",
        "\n",
        "fig = go.Figure()\n",
        "for fold in range(5):\n",
        "    fig.add_trace(go.Scatter(x=list(range(len(moving_avg_rewards))),\n",
        "                             y=moving_avg_rewards[fold],\n",
        "                             mode='lines',\n",
        "                             name=f'Fold {fold+1}'))\n",
        "\n",
        "fig.update_layout(title='Moving Average Rewards per Epoch for each fold',\n",
        "                   xaxis_title='Epoch',\n",
        "                   yaxis_title='Moving Average Rewards')\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "B3bhXdG9YVfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('/content/drive/MyDrive/BERT Models/BERT RL/model')\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/BERT Models/BERT RL/tokenizer')\n",
        "import pickle\n",
        "\n",
        "with open(\"/content/drive/MyDrive/BERT Models/BERT RL/instances\", \"wb\") as f:\n",
        "    pickle.dump(instances, f)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/BERT Models/BERT RL/labels\", \"wb\") as f:\n",
        "    pickle.dump(labels, f)\n"
      ],
      "metadata": {
        "id": "wVtF1gnfxTMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast, DistilBertForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "model_dir = '/content/drive/MyDrive/BERT Models/BERT RL/model'\n",
        "tokenizer_dir = '/content/drive/MyDrive/BERT Models/BERT RL/tokenizer'\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_dir)\n",
        "tokenizer = BertTokenizerFast.from_pretrained(tokenizer_dir)\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/BERT Models/Dataset/unbiasedDataTest.csv')\n",
        "test_instances = test_df['sentence'].tolist()\n",
        "test_labels = test_df['label'].tolist()\n",
        "\n",
        "env = LabelingEnv(test_instances, test_labels)\n",
        "model.eval()\n",
        "preds = []\n",
        "with torch.no_grad():\n",
        "    for instance in test_instances:\n",
        "        encoded = tokenizer([instance], return_tensors='pt', padding='max_length', truncation=True, max_length=128)\n",
        "        encoded = {k: v.to(device) for k, v in encoded.items()}\n",
        "        outputs = model(**encoded)\n",
        "        _, predicted = torch.max(outputs.logits, dim=1)\n",
        "        preds.append(predicted.item())\n",
        "\n",
        "accuracy = accuracy_score(test_labels, preds)\n",
        "precision = precision_score(test_labels, preds)\n",
        "recall = recall_score(test_labels, preds)\n",
        "f1 = f1_score(test_labels, preds)\n",
        "\n",
        "print(f'Test results: Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-score: {f1}\\n')"
      ],
      "metadata": {
        "id": "E1ruvgPMyH0h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}